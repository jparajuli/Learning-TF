{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Tensor Flow : My way"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Here I will be writing my experiences and learning about the tensorflow. \n",
    "\n",
    "Side by side, I will explain some key applications of tensorflow for deep learning.\n",
    "\n",
    "1.  \n",
    "\n",
    "    Tensorflow is an OPEN SOURCE SOFTWARE LIBRARY. \n",
    "    \n",
    "    Very strong for NUMERICAL COMPUTATIONS. \n",
    "    \n",
    "    Use DATA FLOW GRAPHS.\n",
    "\n",
    "    NODES in the graph represent MATHEMATICAL OPERATIONS. \n",
    "    \n",
    "    EDGES represent multidimensional data arrays(TENSORS) communicated betwenn the nodes.\n",
    "\t\n",
    "\n",
    "2.\n",
    "\n",
    "The ARCHITECTURE of Tensorflow is flexible.\n",
    "\t\n",
    "This Architecture allows you to deploy computation to one or more CPUs or GPUs in a desktop, server or \n",
    "mobile device with a single API (Application Programming Interface)\n",
    "\n",
    "\n",
    "3.\n",
    "\n",
    "Tensorflow provides multiple APIs. \n",
    "\n",
    "The lowest level API is called TENSORFLOW CORE.\n",
    "\n",
    "The TF CORE provides you with complete programming control.\n",
    "\n",
    "The TF CORE is recommended for ML Researchers.\n",
    "\n",
    "4.\n",
    "\n",
    "The higher level APIs are easier to use.\n",
    "\n",
    "The higher level APIs make repetitive tasks easier and more consistent between different users.\n",
    "\n",
    "The higher level API such as tf.contrib.learn helps to manage data sets, estimators, trainings and inference.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "What in the world are TENSORS ?\n",
    "\n",
    "In the linear algebraic sense, or mathematically speaking:The Tensor can be represented as a multi-dimensional arrays of the linear values,for the given basis or for the fixed frame of reference.\n",
    "\n",
    "In simple words, TENSORS are the set of primitive values shaped into an array of any number of dimensions.\n",
    "\n",
    "The rank (order or degree) of the tensor is the dimensionality of the array needed to represent it. \n",
    "\n",
    "A linear map is represented by a matrix (2-dimensional array) in a basis. So a matrix is a 2nd order Tensor, whereas a vector is represented by 1 dimension in a basis and is a 1st order Tensor. Scalars are just the numbers and hence are 0th order Tensors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1.,2.,3.] is a rank one Tensor. It is a vector of dimensions $3 \\times 1 $\n",
    "\n",
    "[[1.,2.,3.],[4.,5.,6.]] is a rank two tensor. It is a matrix with dimensions $2 \\times 3 $\n",
    "\n",
    "[[[1.,2.,3.],[4.,5.,6]],[[7.,8.,9.],[10.,11.,12.]]] is a rank three tensor. It is 3 dimensional matrix with dimensiona $(2 \\times 2 \\times 3)$\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Tensorflow core program consists of two discrete functions: \n",
    "\n",
    "1. Building the computational graph\n",
    "\n",
    "2. Running the computational graph"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "What is a computational Graph?\n",
    "\n",
    "They are the series of TF operations arranged into graph of nodes.  \n",
    "\n",
    "Each node takes zero or more tensors as input and produces tensors as output.\n",
    "\n",
    "A type of node called constant takes no tensor as an input and outputs the value it stores internally.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the two floating point tensors that take the constant values and they are represented by two nodes in the computational graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(), dtype=float32) Tensor(\"Const_1:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "node1 = tf.constant(5.0, dtype=tf.float32)\n",
    "node2 = tf.constant(10.0)\n",
    "print(node1, node2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As observed here, the outputs are just the modes of the computational graph and they need to be evaluated to get the exact values. \n",
    "\n",
    "To evaluate the nodes, we must run the computational graph within a session. \n",
    "\n",
    "A session encapsulates the control and state of the Tensorflow runtime.\n",
    "\n",
    "The Session object is created and its run method is run on the computational graph to evaluate the values of the nodes as given by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.0, 10.0]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "print(sess.run([node1, node2]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "More complicated computational graphs are created by combining the Tensor nodes with the operations. The operations are also the nodes. We can add the two nodes, node1 and node2 and produce the new graph as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Add:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "node3 = tf.add(node1, node2)\n",
    "print(node3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.0\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(node3))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Another interesting thing about TF is that it provides a TENSORBOARD which allows us to visualize the computational graph. It means we can observe each nodes and operations in the form of graph."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The graph does not seem so interesting when it produces the constant result all the time. However, the graph can be parameterized to accept the external inputs, known as placeholders. \n",
    "\n",
    "Placeholder is a promise to provide the value later. Let us take the same example as above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", dtype=float32)\n",
      "Tensor(\"Placeholder_1:0\", dtype=float32)\n",
      "Tensor(\"add:0\", dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "node1_ = tf.placeholder(tf.float32)\n",
    "node2_ = tf.placeholder(tf.float32)\n",
    "node3_ = node1_ + node2_\n",
    "print(node1_)\n",
    "print(node2_)\n",
    "print(node3_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.0\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(node3_,{node1_:5., node2_:10.}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  8.  10.]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(node3_,{node1_:[3,4], node2_:[5,6]}))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We can further add more operations and make the computational graph more complex. For example, let us add the multiplication operation in the above case as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"mul:0\", dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "node4_ = node3_ * 5\n",
    "print(node4_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.0\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(node4_,{node1_:5, node2_:10}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, one situation is that the model takes the arbitrary inputs as we provide. The other situation is that, the model takes the same inputs but the graphs need to be modifiable to get the new outputs. \n",
    "\n",
    "Such parameters are called Variables. They are useful to train any models. The variable nodes are constructed with the initial value and the data types as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(1,) dtype=float32_ref> <tf.Variable 'Variable_1:0' shape=(1,) dtype=float32_ref> Tensor(\"Placeholder_2:0\", dtype=float32) Tensor(\"add_1:0\", dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "W = tf.Variable([.3], dtype=tf.float32)\n",
    "b = tf.Variable([-.1], dtype=tf.float32)\n",
    "x = tf.placeholder(tf.float32)\n",
    "\n",
    "#linear model \n",
    "y = W*x + b \n",
    "\n",
    "print(W,b,x,y)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The variables are not initialized when you call tf.Variables. Hence, they need to be initialized. To initialize all the variables in the computational graph, the following operation must be called:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"init\"\n",
      "op: \"NoOp\"\n",
      "input: \"^Variable/Assign\"\n",
      "input: \"^Variable_1/Assign\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "init=tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "print(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.20000002  0.5         0.80000001  1.10000002]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(y,{x:[1,2,3,4]}))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The model is created but we still dont know how good is the model. Next we need to provide the desired values and update the weights and bias. First, let y_ be the placeholder as desired value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_ = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We then write the loss function. Let us consider the loss function which minimizes the squared error between the obtained value from the model and the desired value as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.94\n"
     ]
    }
   ],
   "source": [
    "squared_deltas = tf.square(y_ - y)\n",
    "loss = tf.reduce_sum(squared_deltas)\n",
    "print(sess.run(loss,{x:[1,2,3,4],y_:[0,-1,-2,-3]}))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The variable is initialized using tf.Variable but a new value can be assigned to the variable by using tf.assign as:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "fixW = tf.assign(W, [-1])\n",
    "fixb = tf.assign(b, [1])\n",
    "sess.run([fixW, fixb])\n",
    "print(sess.run(loss,{x:[1,2,3,4], y_:[0,-1,-2,-3]}))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Here, we did not use any machine learning but we were lucky enough to guess the exact value of W and b which minimizes the square error. \n",
    "The whole point of machine learning is to use the algorithms to determine the suitable values of W and b that minimizes the loss function. Loss function can be of any type and also the parameters (variables) can be as many as possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF provides a number of optimizers that slowly changes the value of variables to optimize the loss function. One of the widely used and the simplest optimizers is the Gradient descent optimizer. Tensorflow also obtains the derivative of the given function using the function tf.gradients.  The minimization of loss is obtained as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Loss=[]\n",
    "sess.run(init)\n",
    "for i in range(1000):\n",
    "    sess.run(train, {x:[1,2,3,4], y_:[0,-1,-2,-3]})\n",
    "    We,bi,losss = sess.run([W,b,loss],{x:[1,2,3,4], y_:[0,-1,-2,-3]})\n",
    "    Loss.append(losss)\n",
    "    #print([We, bi, losss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2216128c6a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(Loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The higher level TF library is tf.contrib.learn and now we will explore a bit more about this library:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tf.contrib.learn simplifies the mechanics of machine learning by doing the following :\n",
    "\n",
    "1. running training loops\n",
    "2. running evaluation loops\n",
    "3. managing data sets\n",
    "4. managing feeding\n",
    "\n",
    "Let us look at the linear regression model as defined before with the help of tf.contrib.learn function as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\parajj\\AppData\\Local\\Temp\\tmpw494vhha\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000221650A8F98>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': 'C:\\\\Users\\\\parajj\\\\AppData\\\\Local\\\\Temp\\\\tmpw494vhha'}\n",
      "WARNING:tensorflow:From C:\\Users\\parajj\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\head.py:625: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into C:\\Users\\parajj\\AppData\\Local\\Temp\\tmpw494vhha\\model.ckpt.\n",
      "INFO:tensorflow:loss = 4.75, step = 1\n",
      "INFO:tensorflow:global_step/sec: 1806.53\n",
      "INFO:tensorflow:loss = 0.0651289, step = 101 (0.055 sec)\n",
      "INFO:tensorflow:global_step/sec: 2036.65\n",
      "INFO:tensorflow:loss = 0.00441997, step = 201 (0.049 sec)\n",
      "INFO:tensorflow:global_step/sec: 1824.16\n",
      "INFO:tensorflow:loss = 0.000917031, step = 301 (0.055 sec)\n",
      "INFO:tensorflow:global_step/sec: 1491.09\n",
      "INFO:tensorflow:loss = 0.000737031, step = 401 (0.083 sec)\n",
      "INFO:tensorflow:global_step/sec: 1486.5\n",
      "INFO:tensorflow:loss = 0.000196116, step = 501 (0.052 sec)\n",
      "INFO:tensorflow:global_step/sec: 1312.22\n",
      "INFO:tensorflow:loss = 4.87656e-05, step = 601 (0.076 sec)\n",
      "INFO:tensorflow:global_step/sec: 2042.61\n",
      "INFO:tensorflow:loss = 1.18264e-05, step = 701 (0.049 sec)\n",
      "INFO:tensorflow:global_step/sec: 1879.12\n",
      "INFO:tensorflow:loss = 1.13248e-06, step = 801 (0.053 sec)\n",
      "INFO:tensorflow:global_step/sec: 2619.77\n",
      "INFO:tensorflow:loss = 1.77311e-07, step = 901 (0.038 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into C:\\Users\\parajj\\AppData\\Local\\Temp\\tmpw494vhha\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 4.59933e-08.\n",
      "WARNING:tensorflow:From C:\\Users\\parajj\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\head.py:625: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Starting evaluation at 2017-07-06-10:16:38\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\parajj\\AppData\\Local\\Temp\\tmpw494vhha\\model.ckpt-1000\n",
      "INFO:tensorflow:Finished evaluation at 2017-07-06-10:16:39\n",
      "INFO:tensorflow:Saving dict for global step 1000: global_step = 1000, loss = 4.84977e-08\n",
      "WARNING:tensorflow:From C:\\Users\\parajj\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\head.py:625: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Starting evaluation at 2017-07-06-10:16:39\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\parajj\\AppData\\Local\\Temp\\tmpw494vhha\\model.ckpt-1000\n",
      "INFO:tensorflow:Finished evaluation at 2017-07-06-10:16:40\n",
      "INFO:tensorflow:Saving dict for global step 1000: global_step = 1000, loss = 0.00253996\n",
      "{'loss': 4.8497654e-08, 'global_step': 1000}\n",
      "{'loss': 0.0025399642, 'global_step': 1000}\n"
     ]
    }
   ],
   "source": [
    "# Declare the list of features. Here we consider one real valued feature\n",
    "\n",
    "features = [tf.contrib.layers.real_valued_column('x', dimension=1)]\n",
    "\n",
    "# Now define the estimator. An estimator does the training of data, fitting the model and estimation as well. \n",
    "# We use the linear regression estimator but there are a number of other estimators available. These are \n",
    "#in fact the machine learning algorithms\n",
    "\n",
    "estimator = tf.contrib.learn.LinearRegressor(feature_columns=features)\n",
    "\n",
    "# Next TF has the feature to process the input data. \n",
    "# TF has many helper methods to read and set up the data sets.\n",
    "# Here we use two data sets: one for training and one for evaluation\n",
    "# We need to specify the number of batches of data and the size of each batch\n",
    "\n",
    "x_train = np.array([1.,2.,3.,4.])\n",
    "y_train = np.array([0.,-1.,-2.,-3.])\n",
    "x_eval = np.array([2.,5.,8.,1])\n",
    "y_eval = np.array([-1.01, -4.1, -7., -0])\n",
    "\n",
    "input_fn = tf.contrib.learn.io.numpy_input_fn({'x':x_train},y_train,batch_size=4,num_epochs=1000)\n",
    "\n",
    "eval_fn = tf.contrib.learn.io.numpy_input_fn({'x':x_eval},y_eval,batch_size=4, num_epochs=1000)\n",
    "\n",
    "estimator.fit(input_fn=input_fn, steps=1000)\n",
    "\n",
    "train_loss = estimator.evaluate(input_fn=input_fn)\n",
    "eval_loss = estimator.evaluate(input_fn = eval_fn)\n",
    "\n",
    "print(train_loss)\n",
    "print(eval_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Next we also look at how to build the custom model, which is not built in Tensorflow. Let us analyze on how to construct our own Linear Regressor model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\parajj\\AppData\\Local\\Temp\\tmp8diuvxy5\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000221653BCBE0>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': 'C:\\\\Users\\\\parajj\\\\AppData\\\\Local\\\\Temp\\\\tmp8diuvxy5'}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into C:\\Users\\parajj\\AppData\\Local\\Temp\\tmp8diuvxy5\\model.ckpt.\n",
      "INFO:tensorflow:loss = 123.742932004, step = 1\n",
      "INFO:tensorflow:global_step/sec: 1619.98\n",
      "INFO:tensorflow:loss = 0.131522823812, step = 101 (0.062 sec)\n",
      "INFO:tensorflow:global_step/sec: 1883.36\n",
      "INFO:tensorflow:loss = 0.00244097145656, step = 201 (0.053 sec)\n",
      "INFO:tensorflow:global_step/sec: 2294.68\n",
      "INFO:tensorflow:loss = 0.00104170176225, step = 301 (0.059 sec)\n",
      "INFO:tensorflow:global_step/sec: 1631.03\n",
      "INFO:tensorflow:loss = 6.9962361786e-05, step = 401 (0.046 sec)\n",
      "INFO:tensorflow:global_step/sec: 1976.9\n",
      "INFO:tensorflow:loss = 6.62374113876e-06, step = 501 (0.051 sec)\n",
      "INFO:tensorflow:global_step/sec: 2147.96\n",
      "INFO:tensorflow:loss = 2.10539013076e-07, step = 601 (0.062 sec)\n",
      "INFO:tensorflow:global_step/sec: 1538.9\n",
      "INFO:tensorflow:loss = 2.24484766215e-08, step = 701 (0.049 sec)\n",
      "INFO:tensorflow:global_step/sec: 1926.48\n",
      "INFO:tensorflow:loss = 3.38656293321e-09, step = 801 (0.052 sec)\n",
      "INFO:tensorflow:global_step/sec: 2371.16\n",
      "INFO:tensorflow:loss = 1.09477677844e-10, step = 901 (0.042 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into C:\\Users\\parajj\\AppData\\Local\\Temp\\tmp8diuvxy5\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 3.21481487335e-11.\n",
      "INFO:tensorflow:Starting evaluation at 2017-07-06-11:35:09\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\parajj\\AppData\\Local\\Temp\\tmp8diuvxy5\\model.ckpt-1000\n",
      "INFO:tensorflow:Finished evaluation at 2017-07-06-11:35:10\n",
      "INFO:tensorflow:Saving dict for global step 1000: global_step = 1000, loss = 2.57599e-11\n",
      "INFO:tensorflow:Starting evaluation at 2017-07-06-11:35:10\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\parajj\\AppData\\Local\\Temp\\tmp8diuvxy5\\model.ckpt-1000\n",
      "INFO:tensorflow:Finished evaluation at 2017-07-06-11:35:11\n",
      "INFO:tensorflow:Saving dict for global step 1000: global_step = 1000, loss = 0.0101008\n",
      "train loss: {'loss': 2.5759929e-11, 'global_step': 1000}\n"
     ]
    }
   ],
   "source": [
    "# Declare the list of features, labels and mode and create the function model\n",
    "\n",
    "def model(features, labels, mode):\n",
    "    \n",
    "    # get the variables\n",
    "    W = tf.get_variable('W', [1], tf.float64)\n",
    "    b = tf.get_variable('b', [1], tf.float64)\n",
    "    \n",
    "    #define the linear model\n",
    "    y = W * features['x'] + b\n",
    "    \n",
    "    \n",
    "    #build the loss subgraph\n",
    "    loss = tf.reduce_sum(tf.square(y - labels))\n",
    "    \n",
    "    #buiuld the training subgraph\n",
    "    global_step = tf.train.get_global_step()\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "    train = tf.group(optimizer.minimize(loss),tf.assign_add(global_step,1))\n",
    "    \n",
    "    #We use ModelFnOps to connect subgraphs built for different functionality\n",
    "    return tf.contrib.learn.ModelFnOps(mode=mode, predictions=y,loss=loss, train_op=train)\n",
    "\n",
    "#We then design the estimator for this model\n",
    "\n",
    "estimator = tf.contrib.learn.Estimator(model_fn=model)\n",
    "# define our data sets\n",
    "x_train = np.array([1., 2., 3., 4.])\n",
    "y_train = np.array([0., -1., -2., -3.])\n",
    "x_eval = np.array([2., 5., 8., 1.])\n",
    "y_eval = np.array([-1.01, -4.1, -7, 0.])\n",
    "input_fn = tf.contrib.learn.io.numpy_input_fn({\"x\": x_train}, y_train, 4, num_epochs=1000)\n",
    "eval_fn = tf.contrib.learn.io.numpy_input_fn({\"x\": x_eval}, y_eval, 4, num_epochs=1000)\n",
    "\n",
    "# train\n",
    "estimator.fit(input_fn=input_fn, steps=1000)\n",
    "# Here we evaluate how well our model did. \n",
    "train_loss = estimator.evaluate(input_fn=input_fn)\n",
    "eval_loss = estimator.evaluate(input_fn=eval_fn)\n",
    "print(\"train loss: %r\"% train_loss)\n",
    "#print(\"eval loss: %r\"% eval_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
