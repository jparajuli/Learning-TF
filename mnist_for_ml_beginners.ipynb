{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Just like programming has 'Hello world', machine learning has MNIST (Modified National Institute of Standards and Technology) datasets. In other words, MNIST datasets are the defacto hello world for machine learning programmers. \n",
    "\n",
    "MNIST is a computer vision datasets which consists of the images of handwritten digits and the label for each image telling us what is the handwritten digit. \n",
    "\n",
    "We also use a very simple algorithm called the Softmax regression to analyze them. We also consider each label as one-hot vector. One-hot vector is a vector which has 0 in most dimensions and 1 in single dimension. Hence we represent 2 as: [0,0,1,0,0,0,0,0,0,0]--1 lies in the index 2 and this represents 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 784)\n",
      "(55000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(mnist.train.images.shape)\n",
    "print(mnist.train.labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before describing more about the tutorial, let us understand the \"Softmax Regression\" in detail. Like logistic regression, softmax regression is also a classification problem. However, the logistic regression is a binary class classifier and the softmax regression is a multi-class classifier. Since we need to classify 10 different digits Softmax classifier is very important for MNIST datasets. If we want to assign probabilities to an object being on several different things, softmax can be used because softmax provides the list of values between 0 and 1 that adds upto 1. \n",
    "\n",
    "Very simple idea of the softmax classifier: \n",
    "First add up the evidences of the input being in certain classes.\n",
    "Next convert these evidences into probabilities. \n",
    "\n",
    "To tally up the evidences that the given image is in certain class, we do the weighted sum of the pixel intensities. \n",
    "The weight is negative if the pixel having high intensity is evidence against the image being in that class and the weight is positive if the evidence is in favour.\n",
    "\n",
    "We can also add some extra evidence called Bias. Bias represent those terms or things that are more likely independent of the input. \n",
    "\n",
    "The evidence for class $i$ given an input $x$ is expressed as :\n",
    "\n",
    "$$\\begin{align}\\text{evidence}_i = \\sum_j W_{i,j}x_j + b \\end{align} $$\n",
    "\n",
    "\n",
    "The evidence is then converted into the predicted probabilities by using the softmax function as: \n",
    "\n",
    "$$\\begin{align} y = \\text{softmax(evidence)} \\end{align} $$\n",
    "\n",
    "Softmax is generally defined as the normalized exponential inputs given as: \n",
    "$$\\begin{align}\\text{softmax}(x) = \\text{normalize}(\\exp(x))\\end{align}$$\n",
    "\n",
    "Vectorizing the expression and expressing in terms of matrix and vector form, we can simply write:\n",
    "\n",
    "$$\\begin{align}y = \\text{softmax}(\\mathbf{Wx + b}) \\end{align}, $$\n",
    "\n",
    "where $\\mathbf{W}$ is the weight matrix, $\\mathbf{x}$ is the input vector and $\\mathbf{b}$ is the bias vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Input_data_1:0\", shape=(?, 784), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#we define x as a placeholder and can take any inputs as:\n",
    "x = tf.placeholder(tf.float32,[None, 784],\"Input_data\")\n",
    "\n",
    "#x is not a specific value, but just the placeholder and we will input this value when we ask the Tensorflow to \n",
    "#run the computation\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Weights and biases can be represented as Variables. Variables are the modifiable tensors that lives in Tensorflows graph of interacting operations. It can be used and even modified by the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable_4:0' shape=(784, 10) dtype=float32_ref> <tf.Variable 'Variable_5:0' shape=(10,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "print(W,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Softmax_2:0\", shape=(?, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "y = tf.nn.softmax(tf.matmul(x,W) + b)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous mathematical formulation we obtained y=softmax(Wx+b), but here we did y=softmax(xW+b). This is valid and true because we transpose all the dimensions here, earlier y is expressed as a column vector while y is expressed as a row vector. Mathematically, these two expressions with all the dimensions transposed makes sense. \n",
    "\n",
    "i.e $y^T = (Wx + b)^T = (x^TW^T + b^T)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is training the model we just constructed. The training is performed based on the outputs we have. Let us assign the known outputs by constructing the placeholder tensor y_ as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Known_Output_3:0\", shape=(?, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "y_ = tf.placeholder(tf.float32, [None, 10],'Known_Output')\n",
    "print(y_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the known output, we try to minimize the error which is obtained from our defined model. We use a very nice function called \"Cross-entropy\", which minimizes the loss or error and predicts the optimal values of weights and biases.\n",
    "\n",
    "The cross entropy function is the term arising from information theory which we do not discuss much in machine learning, but it is defined as :\n",
    "\n",
    "$$\\begin{align}H_{y'}(y) = - \\sum_i y'_i\\log (y_i) \\end{align},$$\n",
    "\n",
    "where y' is the true distribution and y is the predicted probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(- tf.reduce_sum(y_*tf.log(y), reduction_indices=[1]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Next we can ask the tensorflow to train the algorithm. We use gradient descent algorithm with certain learning rate as:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.05).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tensorflow then adds the new operation to our computational graph which implement back propagation and the gradient descent and gives back a single operation which when run does a step of gradient descent training, slightly tweaking the variables to reduce the loss. Next we launch the model using an interactive session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "#We can now train the model after initializing the variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Let us run the training steps 1000 times \n",
    "for _ in range(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict = {x: batch_xs, y_:batch_ys})\n",
    "    \n",
    "# In each step of the loop, we get a batch of 100 random data points from the training set. Using such small\n",
    "# batches of random data is called stochastic training-- in this case the stochastic gradient descent. Using \n",
    "# different subsets every time than using all the data is computationally cheap and gives almost similar benefits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let us determine how well does our model do. In the first step we figure out where we predicted the correct labels.\n",
    "\n",
    "tf.argmax is an extremely useful function which gives you the index of the highest entry in a tensor along some axis.\n",
    "\n",
    "We see then that, tf.argmax(y, 1) is the label predicted by the model for each input, however tf.argmax(y_, 1) is the correct label. \n",
    "\n",
    "Thus we can obtain the correct prediction by matching the predicted label with the correct label. This is achieved by using tf.equal() function as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Equal_1:0\", shape=(?,), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "print(correct_prediction)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This gives us a list of boolean values. But we need to determine what fraction of the values are correctly predicted, this can be achieved by casting the boolean values to the floating point expression and taking the mean as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Mean_1:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9011\n"
     ]
    }
   ],
   "source": [
    "# Now we can run and find the accuracy from the given model for our available test data \n",
    "print(sess.run(accuracy, feed_dict={x:mnist.test.images, y_:mnist.test.labels}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
