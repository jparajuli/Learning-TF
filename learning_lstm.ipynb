{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning sequence to sequence neural network algorithm:\n",
    "\n",
    "Before learning sequence to sequence neural network problem, we need to be clear about the recurrent neural network (RNN) and why do we need such neural networks. \n",
    "\n",
    "RNNs are used to make use of the sequential information. In the traditional neural nets, we assume that all the inputs and outputs are independent of each other. But if we want to predict the next input based on the previous inputs or if we want the next sequence based on the previous sequences e.g you want to predict the next word in a sentence based on the previous words, then such traditional networks are not very helpful. In that case, we use the recurrent neural networks.\n",
    "\n",
    "RNNs are called so because they perform the same task on the every element of the sequence with the output being dependent on previous computations. \n",
    "\n",
    "In other words, RNNs have a memory which captures all the information about what has been learned or calculated so far. RNNs can make a long sequence in theory, however they have been used to calculate for smaller number of sequences in practice. \n",
    "\n",
    "<img src=\"rnn.jpg\">\n",
    "\n",
    "\n",
    "RNNs have shown great success in many natural language processing (NLP) tasks.\n",
    "\n",
    "The most commonly used RNNs are LSTMs(Long Short Term Memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will talk something about LSTMs : LSTMs also have the chain like structure like all RNNs but the repeating module has a different structure as shown below:\n",
    "\n",
    "<img src=\"lstm.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four neural network layers, interacting in a very different way as shown in the diagram above. \n",
    "\n",
    "The LSTM does have the ability to remove or add information to the cell state, carefully regulated by structures called gates.\n",
    "\n",
    "Gates are a way to optionally let information through. They are composed out of a sigmoid neural net layer and a pointwise multiplication operation.\n",
    "\n",
    "The sigmoid layer outputs numbers between zero and one, describing how much of each component should be let through. A value of zero means “let nothing through,” while a value of one means “let everything through!”\n",
    "\n",
    "There are three such gates in LSTM. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Forget gate layer: decides what information we are going to throw away from the cell state. It is a sigmoid layer. 1 represents completely keep this and 0 represents completely forget this.\n",
    "\n",
    "2. Input gate layer : decides which values need to be updated. It is also the sigmoid layer. Next the tanh layer creates a vector of new candidate values. \n",
    "\n",
    "3. Output gate layer : decides which part of the cell state we are going to output. First, we run a sigmoid layer which decides what parts of the cell state we’re going to output. Then, we put the cell state through tanh to push the values to be between −1 and 1 and multiply it by the output of the sigmoid gate, so that we only output the parts we decided to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
